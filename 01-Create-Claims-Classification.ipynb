{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "636bf905-4d45-4fdf-8ee7-41493ea5751a",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "022bfec1-9efb-47f1-beb8-586d1d0ccc5a",
   "metadata": {},
   "source": [
    "In this notebook we will learn how to **characterize bus repairs** from free text descriptions, entered by users. This will be accomplished through the use of **Natural Language Processing (NLP)**.\n",
    "\n",
    "This will allow you to discover, step by step, how you can create the code doing the repair text processing.  In the last part of the workshop, this code will be **packaged to create a service** that you can query from an application.\n",
    "\n",
    "We will be training the model on simulated data. (Feel free to explore notebook [00-Generate-Sample-Claims-Data.ipynb](00-Generate-Sample-Claims-Data.ipynb) to see how this data was simulated.)  Once our model is trained, we can test the model by entering a bus repair issue (e.g. the brakes feel soft when I press on them) and check if the model has correctly categorised the claim.  Repairs will be categorized as:  Brakes, Starter or Other.\n",
    "\n",
    "Ready? Let's go!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11cba8fe-4b7d-453b-9cf4-4fe485feccca",
   "metadata": {},
   "source": [
    "# Environment initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cadbffe-164d-40e7-a66e-36c5e0cd4718",
   "metadata": {},
   "source": [
    "## Libraries\n",
    "\n",
    "When we Launched JupyterLab, we selected the `Tensorflow` notebook image. This already has some key libraries installed for us, but we have also added all the libraries (and their versions) which we are relying on into the `nb-requirements.txt` file. This will make it easier for us to repeatably run this code and share it with others. \n",
    "\n",
    "We install these now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0078bfe7-029c-4d47-9538-4d88cbad15e1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in /opt/app-root/lib/python3.9/site-packages (22.2.2)\n",
      "Collecting pip\n",
      "  Downloading pip-24.0-py3-none-any.whl (2.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m54.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: pip\n",
      "  Attempting uninstall: pip\n",
      "    Found existing installation: pip 22.2.2\n",
      "    Uninstalling pip-22.2.2:\n",
      "      Successfully uninstalled pip-22.2.2\n",
      "Successfully installed pip-24.0\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "18e5e797-af85-47fe-80f4-29f67107ad10",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: markovify==0.9.3 in /opt/app-root/lib/python3.9/site-packages (from -r nb-requirements.txt (line 1)) (0.9.3)\n",
      "Requirement already satisfied: tensorflow==2.7.0 in /opt/app-root/lib/python3.9/site-packages (from -r nb-requirements.txt (line 2)) (2.7.0)\n",
      "Requirement already satisfied: scikit-learn==0.24.1 in /opt/app-root/lib/python3.9/site-packages (from -r nb-requirements.txt (line 3)) (0.24.1)\n",
      "Collecting numpy==1.26.4 (from -r nb-requirements.txt (line 4))\n",
      "  Downloading numpy-1.26.4-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: pandas==1.2.4 in /opt/app-root/lib/python3.9/site-packages (from -r nb-requirements.txt (line 5)) (1.2.4)\n",
      "Requirement already satisfied: keras==2.7.0 in /opt/app-root/lib/python3.9/site-packages (from -r nb-requirements.txt (line 6)) (2.7.0)\n",
      "Requirement already satisfied: unidecode in /opt/app-root/lib/python3.9/site-packages (from markovify==0.9.3->-r nb-requirements.txt (line 1)) (1.3.8)\n",
      "Requirement already satisfied: absl-py>=0.4.0 in /opt/app-root/lib/python3.9/site-packages (from tensorflow==2.7.0->-r nb-requirements.txt (line 2)) (2.1.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /opt/app-root/lib/python3.9/site-packages (from tensorflow==2.7.0->-r nb-requirements.txt (line 2)) (1.6.3)\n",
      "Requirement already satisfied: libclang>=9.0.1 in /opt/app-root/lib/python3.9/site-packages (from tensorflow==2.7.0->-r nb-requirements.txt (line 2)) (18.1.1)\n",
      "Requirement already satisfied: flatbuffers<3.0,>=1.12 in /opt/app-root/lib/python3.9/site-packages (from tensorflow==2.7.0->-r nb-requirements.txt (line 2)) (2.0.7)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /opt/app-root/lib/python3.9/site-packages (from tensorflow==2.7.0->-r nb-requirements.txt (line 2)) (0.2.0)\n",
      "Requirement already satisfied: h5py>=2.9.0 in /opt/app-root/lib/python3.9/site-packages (from tensorflow==2.7.0->-r nb-requirements.txt (line 2)) (3.11.0)\n",
      "Requirement already satisfied: keras-preprocessing>=1.1.1 in /opt/app-root/lib/python3.9/site-packages (from tensorflow==2.7.0->-r nb-requirements.txt (line 2)) (1.1.2)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /opt/app-root/lib/python3.9/site-packages (from tensorflow==2.7.0->-r nb-requirements.txt (line 2)) (3.3.0)\n",
      "Requirement already satisfied: protobuf>=3.9.2 in /opt/app-root/lib/python3.9/site-packages (from tensorflow==2.7.0->-r nb-requirements.txt (line 2)) (3.20.3)\n",
      "Requirement already satisfied: six>=1.12.0 in /opt/app-root/lib/python3.9/site-packages (from tensorflow==2.7.0->-r nb-requirements.txt (line 2)) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /opt/app-root/lib/python3.9/site-packages (from tensorflow==2.7.0->-r nb-requirements.txt (line 2)) (2.3.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /opt/app-root/lib/python3.9/site-packages (from tensorflow==2.7.0->-r nb-requirements.txt (line 2)) (4.11.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.32.0 in /opt/app-root/lib/python3.9/site-packages (from tensorflow==2.7.0->-r nb-requirements.txt (line 2)) (0.43.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /opt/app-root/lib/python3.9/site-packages (from tensorflow==2.7.0->-r nb-requirements.txt (line 2)) (1.14.1)\n",
      "Requirement already satisfied: gast<0.5.0,>=0.2.1 in /opt/app-root/lib/python3.9/site-packages (from tensorflow==2.7.0->-r nb-requirements.txt (line 2)) (0.4.0)\n",
      "Requirement already satisfied: tensorboard~=2.6 in /opt/app-root/lib/python3.9/site-packages (from tensorflow==2.7.0->-r nb-requirements.txt (line 2)) (2.15.2)\n",
      "Requirement already satisfied: tensorflow-estimator<2.8,~=2.7.0rc0 in /opt/app-root/lib/python3.9/site-packages (from tensorflow==2.7.0->-r nb-requirements.txt (line 2)) (2.7.0)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.21.0 in /opt/app-root/lib/python3.9/site-packages (from tensorflow==2.7.0->-r nb-requirements.txt (line 2)) (0.36.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /opt/app-root/lib/python3.9/site-packages (from tensorflow==2.7.0->-r nb-requirements.txt (line 2)) (1.62.2)\n",
      "Requirement already satisfied: scipy>=0.19.1 in /opt/app-root/lib/python3.9/site-packages (from scikit-learn==0.24.1->-r nb-requirements.txt (line 3)) (1.9.3)\n",
      "Requirement already satisfied: joblib>=0.11 in /opt/app-root/lib/python3.9/site-packages (from scikit-learn==0.24.1->-r nb-requirements.txt (line 3)) (1.4.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/app-root/lib/python3.9/site-packages (from scikit-learn==0.24.1->-r nb-requirements.txt (line 3)) (3.4.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /opt/app-root/lib/python3.9/site-packages (from pandas==1.2.4->-r nb-requirements.txt (line 5)) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2017.3 in /opt/app-root/lib/python3.9/site-packages (from pandas==1.2.4->-r nb-requirements.txt (line 5)) (2024.1)\n",
      "INFO: pip is looking at multiple versions of scipy to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting scipy>=0.19.1 (from scikit-learn==0.24.1->-r nb-requirements.txt (line 3))\n",
      "  Downloading scipy-1.13.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.6/60.6 kB\u001b[0m \u001b[31m25.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: google-auth<3,>=1.6.3 in /opt/app-root/lib/python3.9/site-packages (from tensorboard~=2.6->tensorflow==2.7.0->-r nb-requirements.txt (line 2)) (2.29.0)\n",
      "Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in /opt/app-root/lib/python3.9/site-packages (from tensorboard~=2.6->tensorflow==2.7.0->-r nb-requirements.txt (line 2)) (1.2.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /opt/app-root/lib/python3.9/site-packages (from tensorboard~=2.6->tensorflow==2.7.0->-r nb-requirements.txt (line 2)) (3.6)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /opt/app-root/lib/python3.9/site-packages (from tensorboard~=2.6->tensorflow==2.7.0->-r nb-requirements.txt (line 2)) (2.31.0)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /opt/app-root/lib/python3.9/site-packages (from tensorboard~=2.6->tensorflow==2.7.0->-r nb-requirements.txt (line 2)) (53.0.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /opt/app-root/lib/python3.9/site-packages (from tensorboard~=2.6->tensorflow==2.7.0->-r nb-requirements.txt (line 2)) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /opt/app-root/lib/python3.9/site-packages (from tensorboard~=2.6->tensorflow==2.7.0->-r nb-requirements.txt (line 2)) (3.0.2)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /opt/app-root/lib/python3.9/site-packages (from google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow==2.7.0->-r nb-requirements.txt (line 2)) (5.3.3)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/app-root/lib/python3.9/site-packages (from google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow==2.7.0->-r nb-requirements.txt (line 2)) (0.4.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /opt/app-root/lib/python3.9/site-packages (from google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow==2.7.0->-r nb-requirements.txt (line 2)) (4.9)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /opt/app-root/lib/python3.9/site-packages (from google-auth-oauthlib<2,>=0.5->tensorboard~=2.6->tensorflow==2.7.0->-r nb-requirements.txt (line 2)) (2.0.0)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in /opt/app-root/lib/python3.9/site-packages (from markdown>=2.6.8->tensorboard~=2.6->tensorflow==2.7.0->-r nb-requirements.txt (line 2)) (7.1.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/app-root/lib/python3.9/site-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow==2.7.0->-r nb-requirements.txt (line 2)) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/app-root/lib/python3.9/site-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow==2.7.0->-r nb-requirements.txt (line 2)) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/app-root/lib/python3.9/site-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow==2.7.0->-r nb-requirements.txt (line 2)) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/app-root/lib/python3.9/site-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow==2.7.0->-r nb-requirements.txt (line 2)) (2024.2.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /opt/app-root/lib/python3.9/site-packages (from werkzeug>=1.0.1->tensorboard~=2.6->tensorflow==2.7.0->-r nb-requirements.txt (line 2)) (2.1.5)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/app-root/lib/python3.9/site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard~=2.6->tensorflow==2.7.0->-r nb-requirements.txt (line 2)) (3.18.1)\n",
      "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /opt/app-root/lib/python3.9/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow==2.7.0->-r nb-requirements.txt (line 2)) (0.6.0)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /opt/app-root/lib/python3.9/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard~=2.6->tensorflow==2.7.0->-r nb-requirements.txt (line 2)) (3.2.2)\n",
      "Downloading numpy-1.26.4-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.2/18.2 MB\u001b[0m \u001b[31m285.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading scipy-1.13.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (38.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m38.6/38.6 MB\u001b[0m \u001b[31m254.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: numpy, scipy\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.19.3\n",
      "    Uninstalling numpy-1.19.3:\n",
      "      Successfully uninstalled numpy-1.19.3\n",
      "  Attempting uninstall: scipy\n",
      "    Found existing installation: scipy 1.9.3\n",
      "    Uninstalling scipy-1.9.3:\n",
      "      Successfully uninstalled scipy-1.9.3\n",
      "Successfully installed numpy-1.26.4 scipy-1.13.0\n"
     ]
    }
   ],
   "source": [
    "!pip install -r nb-requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53cd4352-a1a1-4c9e-b1f3-7658186c591b",
   "metadata": {},
   "source": [
    "## Imports\n",
    "\n",
    "Now that our libraries are installed, we need to import them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ee87a88b-e1bc-4bf9-b756-818c8bb69e3f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-22 11:21:59.585252: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
      "2024-05-22 11:21:59.585292: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c48f6f8e-6f58-4d47-b818-c6983e9e73fc",
   "metadata": {},
   "source": [
    "## Create Training and Testing data sets\n",
    "\n",
    "Now that we have loaded the libraries we need, the first step in our journey is to be able to take our raw data and divide it into testing and training sets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "693d83e0-8968-4ad8-9fea-56d881637b5f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    When turning, the car evaluated.    other\n",
      "0  Cannot get the car doesn't start and doesn't r...  starter\n",
      "1                The car won't shift out of the car.    other\n",
      "2  It is more squealing or screeching sounds than...   brakes\n",
      "3     Front right corner is dented from a collision.    other\n",
      "4     my brakes make a terrible sound when stopping.   brakes\n"
     ]
    }
   ],
   "source": [
    "#============================================================================\n",
    "#Determine what the training and testing percentages, of the data set, will be.\n",
    "#============================================================================\n",
    "training_portion = .80         # Use 80% of data for training, 20% for testing\n",
    "max_words        = 1000        # Max words in text input\n",
    "\n",
    "data             = pd.read_csv('dataset/testdata1.csv')\n",
    "\n",
    "#============================================================================\n",
    "# TRY:  uncomment the below print statement and print out first 5 rows of \n",
    "# generated claims so you can see what data looks like.\n",
    "#\n",
    "print(data.head())\n",
    "#============================================================================\n",
    "\n",
    "train_size       = int(len(data) * training_portion)\n",
    "\n",
    "#============================================================================\n",
    "# FUNCTION:  train_test_split\n",
    "# This function splits the data into training and test sets.  \n",
    "# Inputs:   raw data and determined train_size.\n",
    "#============================================================================\n",
    "def train_test_split(data, train_size):\n",
    "    train        = data[:train_size]\n",
    "    test         = data[train_size:]\n",
    "    return train, test\n",
    "\n",
    "train_cat, test_cat   = train_test_split(data.iloc[:,1], train_size)  # label data is second column\n",
    "train_text, test_text = train_test_split(data.iloc[:,0], train_size)  # text data is first column\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91e0af16-5205-4479-84f8-8dce26e328b0",
   "metadata": {},
   "source": [
    "## Tokenize the Data sets\n",
    "\n",
    "After we have training and testing sets, we need to **tokenize the data**.  This means that we convert text documents into contextual vectors which contain numeric representations (index of where those words occur in a word dictionary) of the words in the documents.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a4d01f6a-4e86-4ce6-bc37-b31b7747a877",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 1. 0. ... 0. 0. 0.]\n",
      " [0. 1. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 1. 0. ... 0. 0. 0.]\n",
      " [0. 1. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "tokenize              = Tokenizer(num_words=max_words, char_level=False)\n",
    "tokenize.fit_on_texts(train_text) # fit tokenizer to our training text data\n",
    "\n",
    "#============================================================================\n",
    "#x_train and x_test are the vectorization of the text data (which is a claim)\n",
    "#============================================================================\n",
    "x_train               = tokenize.texts_to_matrix(train_text)\n",
    "x_test                = tokenize.texts_to_matrix(test_text)\n",
    "\n",
    "#============================================================================\n",
    "# TRY:  uncomment the below print statement and observe the rows in the \n",
    "# newly created matrix.\n",
    "#\n",
    "print(x_train)\n",
    "# ===========================================================================\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d9cd957-445a-4a28-a6e6-3c801304e277",
   "metadata": {},
   "source": [
    "Before we can make a prediction, we will need to pass any future data through the tokenizer to transform it into feature vectors. We now save the tokenizer so we can use it later: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2234b1a5-7f0b-40bd-86d8-3cad5d2a6747",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ae0144e8-413e-42ae-9ef0-1e55a7dfad46",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open('tokenizer.pickle', 'wb') as handle:\n",
    "    pickle.dump(tokenize, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10d4dcc9-c760-465d-9d18-bd274f66c7d3",
   "metadata": {},
   "source": [
    "## Using Scikit-learn\n",
    "We will be using the Label Encoder utility from Scikit-learn to convert label strings to numbered index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3a64ce63-9c28-410d-b1c9-251a61b0c3c7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 1 0 1 0 1 1 2 1 2 1 0 2 0 2 1 1 2 1 1 0 0 0 1 1 0 2 1 0 1 2 0 1 2 1 1 1\n",
      " 1 0 1 1 2 2 1 0 2 1 0 0 2 1 0 1 0 1 1 0 2 1 1 1 2 1 1 1 0 1 1 1 1 0 0 1 2\n",
      " 0 2 2 2 0 1 0 2 1 1 2 1 1 0 0 1 1 1 1 2 1 2 1 0 2 0 1 1 2 1 1 2 1 2 1 0 0\n",
      " 2 1 1 0 0 1 0 2 1 1 1 1 0 1 1 1 2 2 2 1 1 1 1 1 0 2 1 1 2 1 1 2 1 0 2 2 1\n",
      " 0 1 1 1 1 2 0 1 0 2 1 1 1 1 1 1 1 0 1 0 1 2 1 0 1 2 1 0 1 2 2 0 1 0 0 1 2\n",
      " 2 0 1 0 0 1 1 2 1 0 2 2 0 0 1 1 1 2 0 2 2 1 0 1 0 1 1 1 1 0 0 1 1 1 0 1 1\n",
      " 0 1 1 1 0 1 0 2 1 1 1 2 0 0 1 2 0 0 2 1 1 1 2 1 0 2 1 0 1 1 1 2 1 0 0 1 0\n",
      " 2 0 1 1 1 0 1 1 1 1 1 0 1 1 0 2 2 1 2 1 0 1 1 2 2 1 2 2 1 1 1 1 0 2 1 1 2\n",
      " 1 0 0 1 1 1 1 0 1 0 1 2 1 1 0 1 1 2 1 1 0 2 2 2 1 2 1 1 1 2 0 1 2 1 1 2 1\n",
      " 0 2 0 1 1 0 2 2 2 1 2 1 1 0 2 1 1 1 0 1 2 1 1 1 2 2 2 2 2 2 1 1 0 2 1 0 2\n",
      " 1 1 2 1 2 2 2 1 2 1 0 0 1 0 1 1 2 2 0 2 1 0 2 2 1 2 1 0 1 1 1 1 1 2 1 0 1\n",
      " 2 0 2 1 2 2 1 2 1 0 1 1 1 0 1 0 1 1 1 1 1 1 1 1 1 0 1 1 0 1 1 1 2 1 1 0 1\n",
      " 1 2 2 1 1 0 0 2 2 2 1 1 2 0 0 1 0 0 1 1 0 0 1 0 0 2 1 1 0 2 0 1 2 1 0 0 1\n",
      " 0 0 0 1 1 1 1 2 0 1 1 0 0 1 1 1 1 0 1 1 1 2 1 2 1 0 1 0 0 1 1 2 1 1 0 0 2\n",
      " 0 2 2 0 1 1 1 1 2 1 1 2 0 2 2 2 2 1 0 0 1 1 1 0 2 2 1 1 1 2 1 1 1 1 2 1 1\n",
      " 1 1 2 1 2 1 2 0 1 2 1 0 1 2 1 1 2 1 2 2 1 0 1 1 2 1 2 1 1 2 1 1 1 1 1 0 1\n",
      " 1 1 1 1 2 0 2 0 0 2 0 2 0 1 1 1 1 1 1 2 2 0 2 2 1 1 1 2 2 2 0 1 1 1 1 0 0\n",
      " 1 0 1 0 2 1 0 0 1 0 2 2 0 2 1 1 1 1 0 1 2 1 1 1 0 1 2 2 1 2 0 0 1 0 1 2 1\n",
      " 0 1 0 1 0 1 0 1 1 1 1 2 2 1 1 1 0 2 0 2 1 2 0 2 0 2 1 2 1 2 1 0 1 1 0 1 0\n",
      " 1 0 0 2 1 0 2 1 1 2 2 0 1 0 1 1 2 1 1 1 1 0 2 0 0 0 1 1 1 0 1 1 1 0 2 0 2\n",
      " 0 0 1 1 1 1 1 2 1 1 2 1 1 1 2 2 1 2 2 2 0 1 0 1 1 1 1 1 0 1 0 1 2 1 1 0 1\n",
      " 2 0 1 1 2 1 0 2 1 1 1 1 1 1 1 1 1 1 0 0 1 0]\n"
     ]
    }
   ],
   "source": [
    "#============================================================================\n",
    "# Convert label strings to numbered index\n",
    "#============================================================================\n",
    "encoder              = LabelEncoder()  \n",
    "encoder.fit(train_cat)\n",
    "y_train              = encoder.transform(train_cat)\n",
    "y_test               = encoder.transform(test_cat)\n",
    "\n",
    "#============================================================================\n",
    "# Note: for each row in the data, each entry represents the value of the label\n",
    "# Example:  [2 1 1 2 1 1 0 ...  which corresponds to starter, other,\n",
    "# other, starter, other, other, brakes ...\n",
    "#\n",
    "#============================================================================\n",
    "# TRY:  uncomment the below print statement.  What would you expect y_train\n",
    "# to look like?  \n",
    "#\n",
    "print(y_train)\n",
    "#============================================================================\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfcf16bf-c977-4900-8aa0-e45a719141e2",
   "metadata": {},
   "source": [
    "## One Hot Encoding\n",
    "\n",
    "We need to create labels (categories such as Brakes or Starter) for our test data, convert the labels to numbered index and then use one-hot encoding.\n",
    "\n",
    "**One hot encoding** allows the representation of categorical data to be more expressive. Many machine learning algorithms cannot work with categorical data directly. **The categories must be converted into numbers**. This is required for both input and output variables that are categorical.\n",
    "\n",
    "After we have converted the labels using one-hot encoding, we will be ready to build our main NLP model and train it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "26517820-122d-41e3-97f1-ef3b4a09289e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (799, 1000)\n",
      "x_test shape: (200, 1000)\n",
      "y_train shape: (799, 3)\n",
      "y_test shape: (200, 3)\n"
     ]
    }
   ],
   "source": [
    "#============================================================================\n",
    "# Convert the labels to a one-hot representation\n",
    "#\n",
    "# One Hot Encoding replaces the column of labels whose (values are 0 or 1 or 2)\n",
    "# with 3 columns each representing 1 label value.  For example, the label \n",
    "# 'other' is replaced by the vector 0 1 0, the label 'starter' is replaced by\n",
    "# the vector 0 0 1, the label 'brakes' is replaced by the vector 1 0 0\n",
    "#============================================================================\n",
    "num_classes          = len(set(y_train))  # set() creates a unique set of objects\n",
    "y_train              = to_categorical(y_train, num_classes)  \n",
    "y_test               = to_categorical(y_test, num_classes)\n",
    "\n",
    "#============================================================================\n",
    "# TRY:  uncomment the below print statements in order to inspect the \n",
    "# dimenstions of our training and test data.\n",
    "# y_train may appear as y_train shape: (159, 3) which represents 159 rows, 3 cols\n",
    "#\n",
    "print('x_train shape:', x_train.shape)\n",
    "print('x_test shape:', x_test.shape)\n",
    "print('y_train shape:', y_train.shape)\n",
    "print('y_test shape:', y_test.shape)\n",
    "#============================================================================\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88a6416d-6cd6-49d8-9aca-2379b0702a43",
   "metadata": {},
   "source": [
    "\n",
    "## Building the model\n",
    "\n",
    "Once the model is trained, we can test our model by entering a repair issue (e.g. the brakes feel soft when I press on them) and check if the model has correctly characterized the repair issue. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dc05d7a6-7e54-4208-b743-0ffbb866dec0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "WARNING:tensorflow:Model was constructed with shape (None, None, 1000) for input KerasTensor(type_spec=TensorSpec(shape=(None, None, 1000), dtype=tf.float32, name='dense_input'), name='dense_input', description=\"created by layer 'dense_input'\"), but it was called on an input with incompatible shape (None, 1000).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-22 11:26:04.852935: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
      "2024-05-22 11:26:04.852974: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2024-05-22 11:26:04.853014: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (jupyter-nb-thomas-2dbiesmans-0): /proc/driver/nvidia/version does not exist\n",
      "2024-05-22 11:26:04.853234: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Model was constructed with shape (None, None, 1000) for input KerasTensor(type_spec=TensorSpec(shape=(None, None, 1000), dtype=tf.float32, name='dense_input'), name='dense_input', description=\"created by layer 'dense_input'\"), but it was called on an input with incompatible shape (None, 1000).\n",
      " 1/23 [>.............................] - ETA: 7s - loss: 1.0837 - accuracy: 0.4062WARNING:tensorflow:Model was constructed with shape (None, None, 1000) for input KerasTensor(type_spec=TensorSpec(shape=(None, None, 1000), dtype=tf.float32, name='dense_input'), name='dense_input', description=\"created by layer 'dense_input'\"), but it was called on an input with incompatible shape (None, 1000).\n",
      "23/23 [==============================] - 1s 10ms/step - loss: 0.7964 - accuracy: 0.7427 - val_loss: 0.4990 - val_accuracy: 0.9250\n",
      "Epoch 2/2\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 0.3135 - accuracy: 0.9764 - val_loss: 0.2257 - val_accuracy: 0.9750\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 0.2033 - accuracy: 0.9800\n",
      "Test loss: 0.2032698392868042\n",
      "Test accuracy: 0.9800000190734863\n"
     ]
    }
   ],
   "source": [
    "#============================================================================\n",
    "# Build model\n",
    "#============================================================================\n",
    "layers               = keras.layers\n",
    "models               = keras.models\n",
    "model                = models.Sequential()\n",
    "#model.add(layers.Dense(512, input_shape=(max_words,), activation='relu'))  # Hidden layer with 512 nodes\n",
    "model.add(layers.Dense(512, input_shape=(None,1000), activation='relu'))  # Hidden layer with 512 nodes\n",
    "model.add(layers.Dense(num_classes, activation='softmax'))\n",
    "\n",
    "#============================================================================\n",
    "# relu, softmax, categorical_crossentropy are telling the model how to do some \n",
    "# internal calculations.  Softmax is telling the model to calculate \n",
    "# probabilities for each category in each document.  If you only had yes, \n",
    "# or no you would use sigmoid instead of softmax.\n",
    "#============================================================================\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "#============================================================================\n",
    "# VARIABLES\n",
    "# history    - normally used to plot learning curves.  \n",
    "# fit        - calculates the weights in the model. \n",
    "# batch_size - tells the internal calculations how many rows to process at 1 time\n",
    "# epochs     - num of times model calculations will pass through the entire data\n",
    "#============================================================================\n",
    "batch_size          = 32\n",
    "epochs              = 2\n",
    "history = model.fit(x_train, y_train,      \n",
    "                    batch_size=batch_size,  \n",
    "                    epochs=epochs,         \n",
    "                    verbose=1,\n",
    "                    validation_split=0.1)\n",
    "\n",
    "#============================================================================\n",
    "# evaluate func compares the model predictions with the actual known test values\n",
    "#============================================================================\n",
    "score = model.evaluate(x_test, y_test,       \n",
    "                       batch_size=batch_size, verbose=1)\n",
    "\n",
    "#============================================================================\n",
    "# TRY:  uncomment the below print statements to see the test loss and accuracy\n",
    "# of our model\n",
    "#\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])\n",
    "#============================================================================"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7caa08b-cfca-41ad-9a11-281aad8e67e8",
   "metadata": {},
   "source": [
    "We can save our trained model, and our encoder classes, both of which we will need to use later when we create a model service"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "441b36cd-97f4-4ecc-a162-642cf66ef0a5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.save('models/repairmodel.h5')\n",
    "\n",
    "text_labels = encoder.classes_   #ndarray of output values (labels or classes)  e.g. other, brakes, starter\n",
    "\n",
    "with open('text_labels.npy', 'wb') as f:\n",
    "    np.save(f, text_labels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76cecb48-13d4-4387-ac8f-37bfc797c8f4",
   "metadata": {},
   "source": [
    "\n",
    "## Let's test our model!\n",
    "\n",
    "Now that we have a model, we would like to generate a prediction (e.g. categorize the repair issue as:  Brakes, Starter or Other). We must create a prediction function which takes in a string and returns the predicted catagory of the error:\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "157b9c35-7cf9-4302-b489-64fb2df23e65",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "turn the key and nothing happens\n",
      "{'prediction': 'starter'}\n"
     ]
    }
   ],
   "source": [
    "def predict(single_test_text):\n",
    "\n",
    "    text_as_series = pd.Series(single_test_text) #do a data conversion\n",
    "    single_x_test = tokenize.texts_to_matrix(text_as_series)\n",
    "    single_prediction = model.predict(np.array([single_x_test]))\n",
    "    single_predicted_label = text_labels[np.argmax(single_prediction)]\n",
    "    \n",
    "    return {'prediction': single_predicted_label}\n",
    "\n",
    "#========================================\n",
    "#Run the firs time in order to save the model\n",
    "#=========================================\n",
    "single_test_text = 'turn the key and nothing happens' \n",
    "print(single_test_text)    #print out the repair being categorized\n",
    "\n",
    "prediction = predict(single_test_text) \n",
    "print(prediction)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "70e603b3-f4e2-428f-857b-8829d4ba270f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---\n",
      "Brake test\n",
      "press brake pedal and car wont stop\n",
      "{'prediction': 'brakes'}\n",
      "---\n",
      "Starter test\n",
      "turn key over and hear a clicking sound\n",
      "{'prediction': 'starter'}\n",
      "---\n",
      "Other test\n",
      "there is fluid leaking from the engine\n",
      "{'prediction': 'other'}\n"
     ]
    }
   ],
   "source": [
    "#==========================================================================\n",
    "# TRY: uncomment the below to test the predict function.  We will test 3\n",
    "# repair cases so that we can see if the model can properly categorize\n",
    "# a Brake, a Starter and an Other repair text\n",
    "\n",
    "print('---\\nBrake test')\n",
    "single_test_text = 'press brake pedal and car wont stop'\n",
    "print(single_test_text)    #print out the repair being categorized\n",
    "model = keras.models.load_model('models/repairmodel.h5')  #load the model before called predict function\n",
    "prediction = predict(single_test_text)\n",
    "print(prediction)\n",
    "\n",
    "print('---\\nStarter test')\n",
    "single_test_text = 'turn key over and hear a clicking sound' \n",
    "print(single_test_text)    #print out the repair being categorized\n",
    "model = keras.models.load_model('models/repairmodel.h5')  #load the model before called predict function\n",
    "prediction = predict(single_test_text)\n",
    "print(prediction)\n",
    "\n",
    "print('---\\nOther test')\n",
    "single_test_text = 'there is fluid leaking from the engine' \n",
    "print(single_test_text)    #print out the repair being categorized\n",
    "model = keras.models.load_model('models/repairmodel.h5')  #load the model before called predict function\n",
    "prediction = predict(single_test_text)\n",
    "print(prediction)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a000459-a5f9-40e9-a0b5-d1e8a6baa1f3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
